app:
  ai:
    provider: ollama  # choose between ollama, openai, azure
spring:
  application:
    name: spring-ai-intro
  ai:
    chat:
      client:
        enabled: false # disable auto configuration of ChatClient, there are multiple LLM implementations
    # active configuration see app.ai.provider
    ollama:
        base-url: http://localhost:11434/
        chat:
          options:
            model: llama3.2
        embedding:
          options:
            model: llama3.2
    # configured but not used see OpenAiConfig.java
    openai:
        base-url: https://api.openai.com/
        api-key: ${LLM_API_KEY}
        chat:
          options:
            model: llama3-8b-8192
        embedding:
          options:
            model: text-embedding-ada-002
   # end of llms configuration
    vectorstore:
      redis:
        initialize-schema: true