app:
  ai:
    provider: ollama  # choose between ollama, openai, azure
spring:
  application:
    name: spring-ai-intro
  ai:
    chat:
      client:
        enabled: false # disable auto configuration of ChatClient, there are multiple LLM implementations
    # active configuration see app.ai.provider
    ollama:
        base-url: http://localhost:11434/
        chat:
          options:
            model: llama3.2
        embedding:
          options:
            model: llama3.2
   # end of llms configuration
    vectorstore:
      redis:
        initialize-schema: true